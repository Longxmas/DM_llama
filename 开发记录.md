下面是基于详细解释添加具体函数的完整调用流程图，包含所有核心函数和它们的调用关系：

---

### 推理流程调用图

```plaintext
llama_load_model()  // 加载模型和初始化
  ├── ggml_context_init()          // 初始化内存上下文
  ├── load_weights()               // 加载模型权重
  │     ├── ggml_new_tensor()      // 分配权重张量（未量化）
  │     └── 文件I/O操作 (std::ifstream 等)
  ├── kv_cache_init()              // 初始化 KV Cache
  │     └── ggml_new_tensor()      // 为 Key 和 Value 分配存储
  └── llama_vocab_tokenize_init()  // 初始化分词器

llama_eval()  // 推理主流程
  ├── llama_tokenize()             // 输入文本编码
  │     └── llama_vocab_tokenize() // 调用词汇表进行分词
  ├── ggml_compute_forward()       // 逐层前向计算
  │     ├── Embedding 层
  │     │     └── ggml_mul_mat()   // 嵌入矩阵乘法
  │     ├── Transformer 层（遍历每一层）
  │     │     ├── Attention 模块
  │     │     │     ├── ggml_mul_mat()    // 计算 Query, Key, Value
  │     │     │     ├── ggml_softmax()    // 对注意力权重归一化
  │     │     │     └── ggml_mul_mat()    // 矩阵乘法生成注意力输出
  │     │     ├── Feed-Forward 模块
  │     │     │     ├── ggml_mul_mat()    // 第一层全连接
  │     │     │     ├── ggml_gelu()       // 激活函数 (GELU)
  │     │     │     └── ggml_mul_mat()    // 第二层全连接
  │     │     └── Residual + LayerNorm
  │     │           ├── ggml_add()       // 残差连接
  │     │           └── ggml_norm()      // LayerNorm 归一化
  │     └── 输出层
  │           └── ggml_mul_mat()         // 将输出映射到词汇表维度
  ├── kv_cache_update()                  // 更新 KV Cache
  │     ├── ggml_copy()                  // 追加新 KV 数据
  │     └── ggml_view()                  // 管理缓存内存视图
  └── llama_sample()                     // 根据 logits 解码下一个 Token
        ├── logits_to_probabilities()   // 计算概率分布
        └── random_choice()             // 从分布中采样 Token

llama_free_model()  // 释放资源
  ├── ggml_free()                     // 清理内存上下文
  ├── 清理 KV Cache
  └── 清理词汇表和其他配置
```

---

### 各模块函数作用总结

#### 1. **初始化阶段**
- **`ggml_context_init()`**: 初始化内存管理，创建计算图上下文。
- **`load_weights()`**: 从磁盘加载模型权重。
  - **`ggml_new_tensor()`**: 为模型的参数（如权重矩阵）分配张量。

#### 2. **推理阶段**
- **`llama_tokenize()`**: 将输入文本转为 Token ID。
  - **`llama_vocab_tokenize()`**: 根据模型词汇表进行分词。
- **`ggml_compute_forward()`**: 逐层计算模型的前向传播。
  - **Embedding 层**: 计算词嵌入。
    - 调用 **`ggml_mul_mat()`** 执行矩阵乘法。
  - **Transformer 层**: 包括 Attention 和 FFN 模块。
    - **Attention**:
      - **`ggml_mul_mat()`**: 计算 Query、Key、Value 矩阵。
      - **`ggml_softmax()`**: 对注意力权重进行归一化。
      - **`ggml_mul_mat()`**: 生成注意力输出。
    - **Feed-Forward Network (FFN)**:
      - **`ggml_mul_mat()`**: 第一层全连接。
      - **`ggml_gelu()`**: 激活函数。
      - **`ggml_mul_mat()`**: 第二层全连接。
    - **Residual + LayerNorm**:
      - **`ggml_add()`**: 叠加残差。
      - **`ggml_norm()`**: 归一化处理。
  - **输出层**:
    - **`ggml_mul_mat()`**: 映射到词汇表维度。
- **`kv_cache_update()`**:
  - **`ggml_copy()`**: 将新的 Key 和 Value 添加到缓存中。
  - **`ggml_view()`**: 创建视图管理缓存。
- **`llama_sample()`**: 根据 logits 输出采样生成下一个 Token。

#### 3. **清理阶段**
- **`llama_free_model()`**:
  - **`ggml_free()`**: 释放所有分配的内存，包括模型权重、KV Cache 和上下文。

---

### 注意事项
1. **未量化流程**：
   - 不涉及量化的特殊逻辑，仅使用全精度张量操作。
   - KV Cache、权重矩阵和注意力输出都保留高精度存储。

2. **高效优化点**：
   - **`kv_cache_update()`** 可结合稀疏优化。
   - 减少 `ggml_mul_mat()` 中的冗余计算，如 Key 和 Value 的重复计算。

此流程完整涵盖了未量化模式下 `llama.cpp` 的推理逻辑，结合源码中的关键函数可进一步调整优化。